{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mdBU5Hgrm-a"
      },
      "source": [
        "## **Deep Learning for Causal Inference by Vikas Ramachandra**\n",
        "\n",
        "Applying the concepts from the \"Deep Learning for Causal Inference\" paper authored by Vikas Ramachandra to the data_for_churn_analysis dataset\n",
        "\n",
        "link to the paper: <a>https://arxiv.org/abs/1803.00149</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zvpIJP04rm-c",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# DA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ML\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_percentage_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8SYz7N7arm-c",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Loading dataset\n",
        "df = pd.read_csv('data_for_churn_analysis.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQKDeQ46rm-c",
        "metadata": {},
        "outputId": "a070e69c-235e-4fb1-a49e-c49882c49c04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(104143, 18)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX90IDW1rm-d",
        "metadata": {},
        "outputId": "16c3563e-0cbd-4ea1-a88a-6accff434aba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 104143 entries, 0 to 104142\n",
            "Data columns (total 18 columns):\n",
            " #   Column                                 Non-Null Count   Dtype  \n",
            "---  ------                                 --------------   -----  \n",
            " 0   device                                 104025 non-null  object \n",
            " 1   first_payment_amount                   104143 non-null  int64  \n",
            " 2   age                                    104001 non-null  float64\n",
            " 3   city                                   98301 non-null   object \n",
            " 4   number_of_cards                        103671 non-null  float64\n",
            " 5   payments_initiated                     103671 non-null  float64\n",
            " 6   payments_failed                        103671 non-null  float64\n",
            " 7   payments_completed                     103671 non-null  float64\n",
            " 8   payments_completed_amount_first_7days  103671 non-null  float64\n",
            " 9   reward_purchase_count_first_7days      80879 non-null   float64\n",
            " 10  coins_redeemed_first_7days             103671 non-null  float64\n",
            " 11  is_referral                            104143 non-null  bool   \n",
            " 12  visits_feature_1                       101497 non-null  float64\n",
            " 13  visits_feature_2                       101497 non-null  float64\n",
            " 14  given_permission_1                     104143 non-null  int64  \n",
            " 15  given_permission_2                     104143 non-null  int64  \n",
            " 16  user_id                                104143 non-null  int64  \n",
            " 17  is_churned                             104143 non-null  int64  \n",
            "dtypes: bool(1), float64(10), int64(5), object(2)\n",
            "memory usage: 13.6+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoV-zMYgrm-d",
        "outputId": "ea8ab164-100f-4f4e-c725-8b94394e5c84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df.isnull().sum().sum()=37490\n",
            "perc of dataset missing 0.3599857887712088\n"
          ]
        }
      ],
      "source": [
        "# null values?\n",
        "print(f\"{df.isnull().sum().sum()=}\")\n",
        "print(f\"perc of dataset missing {df.isnull().sum().sum()/df.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgvZy3Ujrm-d",
        "outputId": "9b1e33a6-b7a0-40fe-87de-40e2c0094f4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "is_churned\n",
              "0    0.713192\n",
              "1    0.286808\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# proportion of people who have churned\n",
        "df['is_churned'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EqFfmVArm-d"
      },
      "source": [
        "### 1. **Impact of Referrals on Customer Acquisition and Retention**:\n",
        "   - Research Question: Do customers acquired through referrals (`is_referral`) exhibit different behaviors and retention rates compared to non-referred customers?\n",
        "   - Treatment: Customer acquisition through referrals\n",
        "   - Outcome: Customer behavior (e.g., `payments_initiated`, `payments_completed`, `visits_feature_1`, `visits_feature_2`) and churn (`is_churned`)\n",
        "   - Potential Confounders: `device`, `age`, `city`, `number_of_cards`, `payments_failed`, `payments_completed_amount_first_7days`, `reward_purchase_count_first_7days`, `coins_redeemed_first_7days`, `given_permission_1`, `given_permission_2`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEakGKKlrm-d"
      },
      "source": [
        "$$Y_i = f(T_i, X_i, \\epsilon_i)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $Y_i$ represents the outcome variable `is_churned` for customer $i$\n",
        "- $T_i$ is the treatment variable `is_referral`, indicating whether customer $i$ was acquired through a referral\n",
        "- $X_i$ represents the vector of potential confounding variables for customer $i$, such as `device`, `age`, `city`, `number_of_cards`, `payments_failed`, `payments_completed_amount_first_7days`, `reward_purchase_count_first_7days`, `coins_redeemed_first_7days`, `given_permission_1`, `given_permission_2`\n",
        "- $\\epsilon_i$ is the error term, accounting for unobserved factors affecting the outcome\n",
        "- $f$ is an unknown function that maps the treatment, confounders, and error term to the outcome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xvw_7xl1rm-d"
      },
      "outputs": [],
      "source": [
        "# -------------- DATA PREPROCESSING --------------------\n",
        "\n",
        "# Encoding using label encoding technique\n",
        "obj_cols = df.select_dtypes(include='object').columns # grabs object dtypes columns\n",
        "le = LabelEncoder() # creates LabelEncoder instance\n",
        "for col in obj_cols:\n",
        "    df[col] = le.fit_transform(df[col]) # encodes each column\n",
        "\n",
        "df['is_referral'] = np.where(df['is_referral'] == True, 1, 0)\n",
        "\n",
        "\n",
        "# -------------- IMPUTING MISSING VALUES --------------------\n",
        "missing_cols = df.columns[df.isna().any()].tolist()\n",
        "for col in missing_cols:\n",
        "    df[col] = df[col].fillna(df[col].mean())\n",
        "\n",
        "\n",
        "# -------------- DATA SPLIT --------------------\n",
        "\n",
        "confounders = [\n",
        "    'device',\n",
        "    'age',\n",
        "    'city',\n",
        "    'number_of_cards',\n",
        "    'payments_failed',\n",
        "    'payments_completed_amount_first_7days',\n",
        "    'reward_purchase_count_first_7days',\n",
        "    'coins_redeemed_first_7days',\n",
        "    'given_permission_1',\n",
        "    'given_permission_2',\n",
        "    'is_referral' # treatment\n",
        "]\n",
        "\n",
        "y = df['is_churned']\n",
        "X = df[confounders]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
        "Y_train, Y_test = train_test_split(y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaMEkZ6rrm-e"
      },
      "source": [
        "### 1. **Generalized Neighbor Matching using Autoencoders**\n",
        "\n",
        "The paper proposes using autoencoders, a type of deep neural network, for dimensionality reduction while preserving the local neighborhood structure of the data. This is useful for generalized neighbor matching to estimate individual treatment effects (ITEs).\n",
        "\n",
        "The key points are:\n",
        "\n",
        "* In high dimensions, traditional neighbor matching methods like k-nearest neighbors struggle\n",
        "* Autoencoders can learn a low-dimensional representation that captures the manifold structure\n",
        "* This low-dimensional encoding preserves local neighborhoods for accurate neighbor identification\n",
        "* Experiments show autoencoders outperform methods like manifold learning for ITE estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Manifold Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Autoencoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VastVKpxrm-e"
      },
      "outputs": [],
      "source": [
        "input_dim = X.shape[1]  # Features count\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=(input_dim, ))\n",
        "\n",
        "# Encoder: Reduce dimensionality\n",
        "encoded = Dense(64, activation='relu')(input_layer)\n",
        "encoded = Dense(32, activation='relu')(encoded)\n",
        "\n",
        "# Decoder: Reconstruct the input\n",
        "decoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "# Autoencoder model\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey_7YBXBrm-e",
        "outputId": "976cfc0b-d297-41b1-84e6-bcd9005c5703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 17951.0527 - val_loss: 18847.6953\n",
            "Epoch 2/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 19158.2441 - val_loss: 18847.6562\n",
            "Epoch 3/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18545.9727 - val_loss: 18847.6270\n",
            "Epoch 4/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18116.4785 - val_loss: 18847.5996\n",
            "Epoch 5/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18266.0293 - val_loss: 18847.5957\n",
            "Epoch 6/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18893.5430 - val_loss: 18847.5723\n",
            "Epoch 7/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18223.2090 - val_loss: 18847.5684\n",
            "Epoch 8/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18491.7422 - val_loss: 18847.5645\n",
            "Epoch 9/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18212.2676 - val_loss: 18847.5645\n",
            "Epoch 10/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18069.1270 - val_loss: 18847.5996\n",
            "Epoch 11/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18224.3828 - val_loss: 18847.5586\n",
            "Epoch 12/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18254.8574 - val_loss: 18847.5547\n",
            "Epoch 13/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18731.0137 - val_loss: 18847.5547\n",
            "Epoch 14/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18532.5781 - val_loss: 18847.5527\n",
            "Epoch 15/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18303.3730 - val_loss: 18847.5527\n",
            "Epoch 16/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18384.5508 - val_loss: 18847.5547\n",
            "Epoch 17/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18955.5273 - val_loss: 18847.5645\n",
            "Epoch 18/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18242.5977 - val_loss: 18847.5527\n",
            "Epoch 19/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18552.8750 - val_loss: 18847.6250\n",
            "Epoch 20/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18267.5586 - val_loss: 18847.5527\n",
            "Epoch 21/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18073.2871 - val_loss: 18847.5508\n",
            "Epoch 22/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18245.8262 - val_loss: 18847.5586\n",
            "Epoch 23/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18422.3242 - val_loss: 18847.5566\n",
            "Epoch 24/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18557.1191 - val_loss: 18847.5586\n",
            "Epoch 25/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18408.1562 - val_loss: 18847.5508\n",
            "Epoch 26/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18397.1875 - val_loss: 18847.5664\n",
            "Epoch 27/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18195.7285 - val_loss: 18847.5527\n",
            "Epoch 28/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18282.1855 - val_loss: 18847.5508\n",
            "Epoch 29/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18074.5664 - val_loss: 18847.5547\n",
            "Epoch 30/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18767.3047 - val_loss: 18847.5508\n",
            "Epoch 31/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18177.4980 - val_loss: 18847.5527\n",
            "Epoch 32/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18084.6777 - val_loss: 18847.5508\n",
            "Epoch 33/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18076.4277 - val_loss: 18847.5664\n",
            "Epoch 34/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 17881.5234 - val_loss: 18847.5508\n",
            "Epoch 35/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 17990.6543 - val_loss: 18847.5566\n",
            "Epoch 36/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18393.0840 - val_loss: 18847.5508\n",
            "Epoch 37/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18694.5742 - val_loss: 18847.5547\n",
            "Epoch 38/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 17917.8496 - val_loss: 18847.5508\n",
            "Epoch 39/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18901.8965 - val_loss: 18847.5508\n",
            "Epoch 40/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18733.8594 - val_loss: 18847.5527\n",
            "Epoch 41/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18419.9941 - val_loss: 18847.5664\n",
            "Epoch 42/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18582.8555 - val_loss: 18847.5508\n",
            "Epoch 43/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 17968.7383 - val_loss: 18847.5547\n",
            "Epoch 44/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18218.4180 - val_loss: 18847.5508\n",
            "Epoch 45/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18463.2168 - val_loss: 18847.5508\n",
            "Epoch 46/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 18294.7969 - val_loss: 18847.5508\n",
            "Epoch 47/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18537.7617 - val_loss: 18847.5508\n",
            "Epoch 48/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18296.6934 - val_loss: 18847.5508\n",
            "Epoch 49/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18366.1250 - val_loss: 18847.5527\n",
            "Epoch 50/50\n",
            "\u001b[1m326/326\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 18502.8691 - val_loss: 18847.5508\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7012886c7430>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "autoencoder.fit(X_train, X_train,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test, X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjhBqLuo4YZp",
        "outputId": "b91787ee-1bb5-4376-f531-449bb52815e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2604/2604 [==============================] - 7s 3ms/step\n",
            "651/651 [==============================] - 1s 1ms/step\n",
            "3255/3255 [==============================] - 5s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "encoder = Model(input_layer, encoded)\n",
        "X_train_encoded = encoder.predict(X_train)\n",
        "X_test_encoded = encoder.predict(X_test)\n",
        "X_encoded = encoder.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Evaluating Each Method (PCA, Manifold and Autoencoder) based on the paper**\n",
        "\n",
        "\n",
        "Next, to compare B. manifold learning and C. autoencoders, We also compute the estimated treatment effect for each point (ITE), and the average absolute error of ITE for B. manifold learning and C. Autoencoder, over all the data points in the test set.\n",
        "- Mean Absolute error (ITE,autoencoder: 3.7127,\n",
        "- Mean absolute error (ITE, Manifold learning): 4.4540\n",
        "- Thus, autoencoder error is 20.27% lesser than manifold learning estimate for the ITE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYA1U_1Frm-e"
      },
      "source": [
        "### 2. **Deep Neural Networks (DNNs) for Propensity Score Matching**\n",
        "\n",
        "Propensity score matching is a popular technique, but traditionally uses logistic regression for propensity score estimation. The paper proposes using deep neural network classifiers instead, presenting a model called PropensityNet.\n",
        "\n",
        "The key points are:\n",
        "\n",
        "* DNNs can potentially capture complex non-linear relationships better than logistic regression\n",
        "* PropensityNet is trained to estimate propensity scores as a binary classification problem\n",
        "* Experiments show PropensityNet outperforms logistic regression for propensity score estimation\n",
        "* This leads to better matching of treated and untreated units for ITE calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "LF3O2QTIrm-e"
      },
      "outputs": [],
      "source": [
        "# Regular Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter = 10000).fit(X_train_encoded, Y_train)\n",
        "preds = log_reg.predict_proba(X_encoded)\n",
        "\n",
        "df_copy = df.copy()\n",
        "df_copy.loc[:, 'propensity_score'] = preds[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v46JY7e7rm-e",
        "outputId": "0ab03c57-8506-49e9-c979-3b47f94517dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "is_churned\n",
              "0    0.237044\n",
              "1    0.411856\n",
              "Name: propensity_score, dtype: float64"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_copy.groupby('is_churned')['propensity_score'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfCxm6tarm-e",
        "outputId": "5226bc40-87fe-48f5-9cd5-2d8c771083cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7676464092641848"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# assuming a 0.5 threshold\n",
        "accuracy_score(y, (preds[:, 1] > 0.5).astype('int'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4PsLvPprm-e",
        "outputId": "2d4fa9a8-205e-4168-bfec-3131f9db8017"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.23235359073581519"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1 - accuracy_score(y, (preds[:, 1] > 0.5).astype('int'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch-uUHVXrm-e",
        "outputId": "a1c7dec8-37ac-4f9a-9091-a55133ad7b9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(104143, 11)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIC203Xcrm-e"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "iXWe_-h8rm-e"
      },
      "outputs": [],
      "source": [
        "# -------------- PROPENSITY NET --------------------\n",
        "\n",
        "model = nn.Sequential(\n",
        "                nn.Linear(in_features=11, out_features=64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(in_features=64, out_features=64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(in_features=64, out_features=64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(in_features=64, out_features=32),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(in_features=32, out_features=16),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=0.3),\n",
        "                nn.Linear(in_features=16, out_features=1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "\n",
        "# -------------- LOSS FUNCTION (Binaty Cross Entropy) & OPTIMIZER (Adam) --------------------\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# ------- Tensors & Dataloaders --------------\n",
        "# converting to pytorch tensors\n",
        "X_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.to_numpy(), dtype=torch.float32)\n",
        "\n",
        "# Dataloaders\n",
        "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=220, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYYpGGMSrm-e",
        "outputId": "deda9cdb-f257-4ffb-8dee-76ba48c3d0ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 11713\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total number of parameters:\", total_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etTScQ2irm-f",
        "outputId": "25aa841b-507a-4476-a538-3328bff3564c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0 training loss 0.6417275298748338\n",
            "epoch: 1 training loss 0.6035272799715211\n",
            "epoch: 2 training loss 0.6261582033795143\n",
            "epoch: 3 training loss 0.6186024529018482\n",
            "epoch: 4 training loss 0.6038723844516126\n",
            "epoch: 5 training loss 0.6258904630876292\n",
            "epoch: 6 training loss 0.6078912219669246\n",
            "epoch: 7 training loss 0.6118009460123279\n",
            "epoch: 8 training loss 0.6243796401386019\n",
            "epoch: 9 training loss 0.6033857540239261\n",
            "epoch: 10 training loss 0.6184191579305673\n",
            "epoch: 11 training loss 0.6130916148046904\n",
            "epoch: 12 training loss 0.6068442858724151\n",
            "epoch: 13 training loss 0.6232246962151949\n",
            "epoch: 14 training loss 0.6037074091691005\n",
            "epoch: 15 training loss 0.6162571190278742\n",
            "epoch: 16 training loss 0.61226324491863\n",
            "epoch: 17 training loss 0.607340791180164\n",
            "epoch: 18 training loss 0.6207969548958766\n",
            "epoch: 19 training loss 0.6026674624736802\n"
          ]
        }
      ],
      "source": [
        "# -------------- TRAINING LOOP --------------------\n",
        "\n",
        "EPOCHS = 20\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    for inputs, label in train_dataloader:\n",
        "\n",
        "        # ---- FORWARD PASS -----\n",
        "        pred = model(inputs)\n",
        "        pred = pred.squeeze(1)\n",
        "        loss = criterion(pred, label)\n",
        "\n",
        "        # ----- BACKPROPAGATION -----\n",
        "        loss.backward() # calculating gradients\n",
        "        optimizer.step() # multiplies learning_rate * gradient, which is your step size or by how much you update the weights\n",
        "\n",
        "        # loss\n",
        "        epoch_loss += loss.item()\n",
        "    epoch_loss /= len(train_dataloader)\n",
        "    print(\"epoch: {0} training loss {1}\".format(epoch, epoch_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "zB6EVK5_rm-f"
      },
      "outputs": [],
      "source": [
        "# -------------- EVALUATING MODEL --------------------\n",
        "# Make predictions\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "predictions = []\n",
        "with torch.no_grad():  # Disable gradient tracking during inference\n",
        "    for batch in train_dataloader:\n",
        "        inputs, labels = batch\n",
        "        pred = model(inputs)\n",
        "        pred = pred.squeeze(1)  # If your model outputs a single value per sample\n",
        "        predictions.append(pred)\n",
        "\n",
        "\n",
        "# Concatenate predictions into a single tensor\n",
        "predictions_tensor = torch.cat(predictions)\n",
        "\n",
        "# Convert predictions to numpy array\n",
        "predictions_array = predictions_tensor.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwNjgoS2rm-f",
        "outputId": "02851ad3-e9ec-4527-c7e1-e0224a00f6b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.34013438, 0.34013438, 0.34013438, ..., 0.34013438, 0.34013438,\n",
              "       0.34013438], dtype=float32)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzhlGHYcrm-f"
      },
      "source": [
        "#### **Evaluating Each Method (LogisticRegression, PropensityNet) based on the paper**\n",
        "\n",
        "\n",
        "- Evaluate them based on accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "usc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
